\documentclass{book}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{tikz}
\usetikzlibrary{calc,patterns,angles,quotes}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]

\title{Linear spaces}
\author{Joaquín Gómez}

\begin{document}
\frontmatter
\maketitle
\tableofcontents

\pagebreak
\section*{Symbol Glossary}
\begin{tabular}{@{}llp{8.5cm}@{}}
    \textbf{Symbol}     & \textbf{Meaning}             & \textbf{Context / Notes}                                                                \\
    \\[-0.8em] % add a little spacing
    $V$                 & Linear space                 & The main set we're working in                                                           \\
    $O$                 & Zero vector (element of $V$) & Use this consistently for the "zero of the space"                                       \\
    $0$                 & Scalar zero                  & Element of $\mathbb{R}$                                                                 \\
    $I$                 & Identity element             & Scalar $1$ in scalar multiplication or the identity matrix (e.g., $I = \text{diag}(1)$) \\
    $L(S)$              & Linear span of a set $S$     & Smallest subspace containing all linear combinations of $S$                             \\
    $\dim V$            & Dimension of $V$             & Number of elements in a basis for $V$                                                   \\
    $\|x\|$             & Norm (length) of element $x$ & Defined as $\sqrt{(x, x)}$ using the inner product                                      \\
    $(x, y)$            & Inner product of $x$ and $y$ & Dot product in $\mathbb{R}^n$, or more generally defined                                \\
    $x_i$               & Component of vector $x$      & Used in expressions like $x = \sum c_i e_i$                                             \\
    $c_i$               & Scalar coefficient           & Used in linear combinations                                                             \\
    $e_i$               & Basis vector                 & In an ordered basis of $V$                                                              \\
    $P$                 & Projection matrix            & Used for projecting onto subspaces                                                      \\
    $\mathrm{proj}_u v$ & Projection of $v$ onto $u$   & Defined as $\left(\frac{v \cdot u}{\|u\|^2}\right) u$                                   \\
\end{tabular}

\mainmatter
\chapter{Introduction}
\section{Definition of a linear space}
Let $V$ be a non-empty set of objects, called \textit{elements}. The set V is
called linear space if it satisfies the following ten axioms, which are stated
in three groups.

\section*{Axioms of a linear space}

\begin{list}{Axiom}{}
    \item 1.\textit{ Closure property of addition}: for every pair of elements $x,y\in V$, their sum is written as $z=x+y$ and $z\in V$.
    \item 2.\textit{ Closure property of scalar product}: for any $x\in V$ and $a\in\mathbb R$, there is an element $z=ax\in V$.
\end{list}

\subsection*{Axioms for addition}

There are four axioms of addition, we will use a number and a letter to refer
to them. However if we are talking of addition properties we will simply use a
letter to reference them. The same will be done for the axioms of scalar
products.

\begin{list}{Axiom}{}
    \item 3.a.\textit{ Commutative law}: for any $x,y\in V$, we have $x+y = y+x$.
    \item 4.b.\textit{ Associative law}: for any $x,y,z\in V$, we have $(x+y) + z = x + (y + z)$.
    \item 5.c.\textit{ Existence of zero as an element}: there is a number in $V$, designated as $O$ (big `o'), that satisfies
          \[
              x + O = x,\quad \forall x \in V
          \]

    \item 6.d.\textit{ Opposite elements}: for all $x\in V$, the element $(-1)x$ has the property
          \[
              x + (-1)x = O
          \]
\end{list}

\subsection*{Axioms for scalar product}
\begin{list}{Axiom}{}

    \item 7.a.\textit{ Associative law}: for all $x \in V$ and every pair $a,b\in \mathbb R$, we have

          \[
              a(bx) = (ab)x
          \]

    \item 8.b.\textit{ Distributive law for addition in $V$}: for all $x,y\in V$ and $a\in \mathbb{R}$, it is true that

          \[
              a(x+y) = ax+ay
          \]

    \item 9.c.\textit{ Distributive law for addition in $\mathbb R$}: for any $x\in V$ and $a,b\in\mathbb{R}$, we have

          \[
              (a+b)x = ax+bx
          \]

    \item 10.d.\textit{ Existence of an identical element}: for all $x\in V$ theres an unique element $I$ such that $Ix=x$
          (commonly this element is $1$. But, for example, the identical element in matrix spaces is called \textit{identity matrix},
          defined as $I=\textnormal{diag}(1)$)
\end{list}

\section{Examples of linear spaces}

The following examples can be proven to be linear spaces

\begin{enumerate}
    \item Real numbers
    \item The vector space of real numbers $\mathbb R^n$
    \item The set of all matrices
    \item Polynomials $P$ with $\deg P \leq n$ (in this case, if $\deg P = n$, we would
          have a problem with axioms of additions. We can't ensure the sum of two
          polynomials of degree $n$ has degree $n$).
    \item The set of all polynomials
    \item The set of continuous functions on an interval $\left[a, b\right]$. This space
          is designated as $C(a,b)$.
    \item The set of all integrable functions on an interval
    \item The set of differentiable functions on an interval
    \item A plane in $\mathbb R^3$ with the equation $ax+by+cz=0$. Note that this plane
          must always go through the origin to be a linear space.
\end{enumerate}

There are plenty of examples for linear spaces. We can ``create'' a linear
space if we define addition and multiplication for that space.

\section{Consequences of the axioms}

The following theorems are a consequence of the axioms of linear space.

\begin{theorem}[Uniqueness of `O']
    In any linear space there is one and only one zero element
\end{theorem}

\begin{proof}
    Axiom 5 ensures that there is at least one `O' in $V$. Now,
    suppose there are two zeroes in $V$. Let $x = O_1$ and $O_2 = O$,
    thus $x + O = x + O_2 = x = O_1$, but as $O_1$ is zero, $O_1 + O_2 = O_2$,
    this means that $O_1=O_2=O$
\end{proof}

\begin{theorem}[Uniqueness of opposites]
    In any linear space each $x$ has one and only one opposite $y$ such that $x+y=O$
\end{theorem}

\begin{proof}
    Axiom 6 ensures there is at least one opposite of $x$ in $V$. Let $y_1,y_2\in V$
    be two different opposite elements for $x$. Then $x+y_1=O$ and $x+y_2=O$, then

    \[
        (x + y_1) + y_2 = y_2 + O = y_2
    \]

    and

    \[
        y_1 + (x+y_2) = y_1 + O = y_1
    \]

    Thus $y_1 + (x+y_2) = y_1 + (x+y_1) = y_1 + O = O + y_1$, this proves that $y_1
        = y_2$.
\end{proof}

\section{Subspaces of a linear space}

Let $V$ be a linear space and let $S$ be a subset of $V$, if $S$ is also a
linear space, then we say that ``$S$ is a subspace of $V$''.

A subset of a linear space if a subspace only if it satisfies the axioms of
closure.

\begin{theorem}
    Let $V$ be a linear space, if $S\subset V$ and $S \neq \emptyset$ satisfies
    the ten axioms of closure then $S$ is a subspace of $V$.
\end{theorem}

The proof for this theorem is easy, and so I discarded it.

\begin{definition}
    Let $S\subset V$, and $S\neq \emptyset$, where $V$ is a linear space. If $x\in V$ and

    \[
        x=\sum_{i=1}^{k}{c_i x_i}
    \]

    where $x_1,x_2,\dots,x_k \in S$ and $c_1,c_2,\dots,c_k\in \mathbb{R}$, is
    called a \textit{linear combination of elements in $S$}. The set of linear
    combinations of the elements of $S$ satisfies the axioms of closure, so it is
    also a subspace of $V$.\textit{ We say that this subspace is generated by $S$
        and we call it the linear span of $S$, designated by $L(S)$.} If $S=\emptyset$,
    we define $L(S)=\left\{O\right\}$.
\end{definition}

\section{Dependent and independent subsets of a linear space}

In this section we introduce the concept of independence, that is important
when working with systems of linear equations, matrices, and other subjects in
linear algebra.

\begin{definition}
    Let $S$ be a set of elements of a linear space $V$. $S$ is dependent if there
    exists a finite set of distinct elements in $x_1,x_2,\dots,x_k\in S$, and a
    set of scalars $c_1,c_2,\dots,c_k$ where not all of them are zero, that satisfies
    \[
        \sum_{i=1}^{k}{c_i x_i} = 0
    \]

    A set is independent if it is not dependent. So the following

    \[
        \sum_{i=1}^{k}{c_i x_i} = 0, \quad \textit{implies } c_1 = c_2 = \cdots = c_k =0
    \]
\end{definition}

Independency and dependency are properties of sets of elements. However, we can
apply the same concepts to the elements itself. For example, a set of vectors
$v_1, v_2, \dots, v_n \in \mathbb R^{n}$ is called independent if there is
\textbf{not} a linear combination of these vectors that produce the zero
vector.

\begin{example}
    Let $u_k(t)=t^{k}$ for $k=1,2,\dots, n$ and $t\in \mathbb{R}$. The set $V=\{u_1,u_2,\dots, u_n\}$ is independent except in the subset $S$ where $t=-1$ and $n$ is odd.

    \begin{proof}
        For $S$ to be independent, there must be $c_1,c_2,\dots, c_n$, where $c_1 = c_2 = \cdots = c_n =0$ and
        \[
            \sum_{k=0}^{n}{c_k t^{k}} = 0
        \]

        To solve this, we set $c_0=c_1=\cdots=c_n$. If we define

        \[
            f(t)=\sum_{k=0}^{n}{c_k t^{k}},
        \]

        note that $f(-1)=\begin{cases}1\quad \textnormal{if $n$ is even}\\0 \quad \textnormal{if $n$ is odd}\end{cases}$
        We can draw a picture for this problem. Imagine a circle, in which we have two points. We can travel the circumference
        counterclockwise starting from $1$. We start from $1$ because in the case $n=0$, $f(-1)=1$.

        \begin{center}
            \begin{tikzpicture}
                \draw (0,0) circle(1.2cm);
                \draw[fill=black] (1.2,0) circle[radius=2pt];
                \draw[fill=black] (-1.2,0) circle[radius=2pt];
                \draw[->, ultra thick] (0.1,1.2)--(-0.1,1.2);
                \draw[->, ultra thick] (-0.1,-1.2)--(0.1,-1.2);
                \draw (1.5,0) node{1};
                \draw (-1.5,0) node{0};
            \end{tikzpicture}
        \end{center}

        Now start performing counterclockwise turns, and count how many times you go
        from 0 to 1, and from 1 to 0.

        If $C_{0\to 1}$ and $C_{1 \to 0}$ are the counts of going from 0 to 1 and from
        1 to 0 respectively, we have that if $C_{0\to 1}=C_{1\to 0}$, then $f(-1)=1$.
        Otherwise, we must have $f(-1)=0$.

        But having $C_{0\to 1}=C_{1\to 0}$ and that the total count is $C=C_{0\to
            1}+C_{1\to 0}$, means

        \[
            C=2C_{1\to 0}=2C_{0\to 1}
        \]

        Hence, $C$ is an even number. To verify that $S$ is dependent, we set $n=2r-1$
        for any integer $r$ and $t=-1$. With the results above, we can see that

        \[
            \sum_{k=0}^{2r-1}{c_k {(-1)}^{k}}=0
        \]

        if $c_1=c_2=\cdots=c_{2r-1}$, but not necessarily zero. This proves that $S$ is
        dependent.
    \end{proof}
\end{example}

\begin{theorem}
    Let $S=\{x_1,x_2,\dots,x_k\}$ an independent set formed by $k$ elements
    of a linear space $V$ and let $L(S)$ be the linear span of $S$. Then, any
    set of $k+1$ elements from $L(S)$ is dependent.
\end{theorem}

What this theorem says is that, taking any set of vectors in the linear span of
$S$, this is, formed by combining elements of $S$ (this vectors can be of any
nature), then, if we form a subset of the linear span of $S$ and it has more
elements than $S$ itself, the set will be dependent. This is because we are not
providing any new ``dimension'' to the new set. Say $S\in \mathbb{R^{3}}$, as
we are restricted to be in $\mathbb{R^{3}}$, taking $4$ vectors won't make any
object in $R^{i>3}$

\begin{proof}
    Let $T=\{y_1,y_2,\dots,y_{n+1}\}\subset L(S)$, this means that each $y_i$ is
    a linear combination of elements in $S$

    \[
        y_i=\sum_{j=1}^{n}{a_{ij}x_j}, \quad \textnormal{for } i=1,2,\dots,n+1
    \]

    For $T$ to be dependent, there must be some scalar set
    $C=\{c_1,c_2,\dots,c_{n+1}\}$, where not all of them are zero, that satisfies

    \[
        \sum_{i=1}^{n+1}{c_i y_i}=0
    \]

    We now want to prove by induction that for $n-1$ elements of $T$, there is a
    linear combination that satisfies dependency. Thus, we can try to form an
    equation that represents $T$ as a linear combination of $n-1$ elements. For
    this, we are going to take one element of $T$, multiply it by some scalar and
    subtract each element of $T$.

    Take the $1^{st}$ element in $T$ and multiply it by $c_i=\frac{a_{i1}}{a_{11}}$

    \[
        c_i y_1 = a_{i1}x_1 + \sum_{j=2}^{n}{c_i a_{1j} x_j}
    \]

    Now subtract $y_1$

    \begin{equation}
        \begin{split}
            c_i y_1 - y_i= a_{i1}x_1 + \sum_{j=2}^{n}{c_i a_{1j} x_j} - a_{i1}x1 +
            \sum_{j=2}^{n}{a_{ij}x_j}
            \\ = \sum_{j=2}^{n}{c_i a_{1j} x_j - a_{ij}x_j}
            \\ = \sum_{j=2}^{n}{\left(c_i a_{1j}- a_{ij}\right)x_j}
        \end{split}
    \end{equation}

    Equation (1.1) is indeed a linear combination of $n-1$ elements of $S$. By
    induction for $n$, we can prove that there are $n$ scalars
    $t_2,t_3,\dots,t_{n+1}$, that satisfy

    \begin{equation}
        \begin{split}
            \sum_{j=2}^{n+1}{t_i\left(c_i y_1- y_i\right)} = 0
        \end{split}
    \end{equation}

    As each $y_i$ is a linear combination of elements of $S$, we can write $y_i$ in
    terms of $y_1$.

    Equation (1.2) is solvable, because $y_i=c_i y_1$, this is true by the fact
    that $T\subset L(S)$.
\end{proof}

\section{Basis and dimension}
\begin{definition}
    A finite set $S$ of elements of a linear space $V$ is called a \textit{finite basis} of $V$ is $S$ is independent and spans $V$.
    $V$ is of finite dimension if it has a finite basis. Otherwise, $V$ has infinite dimension.
\end{definition}

\begin{theorem}
    Let $V$ be a linear space of finite dimension. Then any finite basis of $V$ has the same number of elements.
\end{theorem}

\begin{proof}
    This theorem can be proved with theorem $1.4$, let $S$ and $T$ be two finite bases for $V$, with $k$ and $m$ elements respectively.
    If $S$ generates $V$, then $V$ must have $k$ elements, we know that any set of $k+1$ elements of $V$ is dependent. Thus, $T$ must have
    $m\geq k$ elements. Applying the same reasoning vice-versa yields that $k=m$.
\end{proof}

This does not mean that a set of $k+1$ elements of $V$ can't span $V$. It
states that, the number of elements for a finite basis of a linear space $V$ of
dimension $k$, must have the same number of elements.

\begin{definition}
    If a linear space $V$ has a finite basis of $n$ elements, we write $n=\dim V$.
\end{definition}

The following theorem will not be proven. However, it has an intuitive
explanation.

\begin{theorem}
    Let $V$ be a linear space of finite dimension, with $\dim V = n$. Then

    \begin{enumerate}
        \item If $S$ is a finite basis for $V$, and $T$ is a set of independent elements of
              $V$, then $T\subseteq S$.
        \item Any set of $n$ independent elements of $V$ is a finite basis for $V$.
    \end{enumerate}

\end{theorem}

\section{Components}

Let $V$ be a linear space with $\dim V = n$, and consider an ordered basis
$\left\{e_1, e_2,\dots,e_n\right\}$. This ordered basis is considered as an
n-tuple $\left(e_1,e_2,\dots,e_n\right)$.

\begin{definition}
    An ordered basis of a linear space $V$ is a set of elements of $V$ that form a basis and provides information
    about the order of its elements.
\end{definition}

If $x\in V$, we can express $x$ as a linear combination of elements of the
basis

\begin{equation}
    \begin{split}
        x=\sum_{i=1}^{n}{c_i e_i}
    \end{split}
\end{equation}

This ensures that there is only one representation of $x$, take $x =
    \sum_{i=1}^{n}{c_i e_i}$, and $x = \sum_{i=1}^{n}{d_i e_i}$. Then

\[
    \sum_{i=1}^{n}{c_i e_i} = \sum_{i=1}^{n}{d_i e_i}
\]

Then $\sum_{i=1}^{n}{(c_i - d_i) e_i} = O$, where $O$ is the zero
vector/element of $V$. This means that $c_i=d_i$ for $i=1,2,\dots,n$. So there
is only one representation of $x$ in $V$.

\chapter{Euclidean spaces, inner products and norms}

We start the section by defining what is a Euclidean space.

\begin{definition}
    A \textit{Euclidean space} is a finite-dimensional linear space that satisfies
    Euclidean geometry. They also are metric spaces, which are sets that have a notion
    of distance between its elements. Euclidean spaces are equipped with an \textit{inner product}.
\end{definition}

Euclidean spaces have a set of properties, that were defined as axioms in
\textit{Euclid's Elements}, which are
\begin{enumerate}
    \item If $a = b$ and $b = c$ then $a = c$ (the transitive property)
    \item If $a = b$ then $a + c = b + c$ (the equal sum property)
    \item If a line segment $\overline{AB}$ coincides in length and direction with
          $\overline{CD}$ then $\overline{AB} = \overline{CD}$.
    \item The whole is greater than the part. This can be thought as: \textit{let $A$ and
              $B\subset A$ be two arbitrary sets, then $A$ is ``bigger'' than $B$}.
    \item Things that are double of the same thing are equal to each other. (This one is
          very obvious, consider two equal circles with radius $r_1$ and $r_2$, then we
          can say that $r_1=r_2$).
\end{enumerate}

\section{Dot product and inner product}

\begin{definition}
    The \textit{inner product} is a function that maps two elements $x$ and $y$ from a linear space
    $V$ to a real number. We write the inner product as $\left(x,y\right)$.

    Any inner product satisfies the following properties:

    \begin{enumerate}
        \item $\left(x,y\right) = \overline{\left(y,x\right)}$ (hermitian symmetry)
        \item $\left(x,y+z\right) = \left(x,y\right) + \left(x,z\right)$ (linearity)
        \item $c\left(x,y\right) = \left(cx,y\right)$ (homogeneity)
        \item $\left(x,x\right) \geq 0$ (positive definite)
    \end{enumerate}

    \textbf{Remember}: a linear space with inner product is called a Euclidean space.
\end{definition}

\begin{example}[Inner product of two vectors in $\mathbb{R}^{2}$]

    \begin{center}
        \begin{tikzpicture}
            \coordinate (u1) at (-2,3);
            \coordinate (u2) at (3,2);
            \coordinate (O) at (0,0);
            \coordinate (x) at (5,0);
            \coordinate (y) at (0,5);

            \draw[->, ultra thick] (O)--(x);
            \draw[->, ultra thick] (O)--(y);

            \draw[->, thick] (O)--(u1);
            \draw[->, thick] (O)--(u2);

            \draw (u1) + (.2,.2) node{$u$};
            \draw (u2) + (.2,.2) node{$v$};
            \pic [draw, ->, "$\theta$", angle eccentricity=1.5] {angle = x--O--u1};
            \pic [draw, ->, "$\phi$", angle eccentricity=2] {angle = x--O--u2};
        \end{tikzpicture}
    \end{center}

    Now, we have $u=\left(u_{x},u_{y}\right)$ and $v=\left(v_x, v_y\right)$. If we
    define the inner product of two vectors as

    \[
        \left(u,v\right) = u \cdot v = \sum_{i=1}^{n}{u_i v_i}
    \]

    For $n=2$, we have $u \cdot v = u_x v_x + u_y v_y$. We know the following
    relationships

    \begin{equation}
        \begin{split}
            \cos{\theta} = \frac{u_x}{|u|},\qquad
            \sin{\theta} = \frac{u_y}{|u|}
            \\\\
            \cos{\phi} = \frac{v_x}{|v|},\qquad
            \sin{\phi} = \frac{v_y}{|v|}
        \end{split}
    \end{equation}

    If we solve for $u$ and $v$ and substitute in the inner product formula, we get

    \begin{equation}
        \begin{split}
            u\cdot v = |u|\cdot|v|\cos\theta\cos\phi + |u|\cdot|v|\sin\theta\sin\phi
            \\ = |u||v|(\cos\theta\cos\phi + \sin\theta\sin\phi)
            \\ = |u||v|(\cos{\left(\theta - \phi\right)})
        \end{split}
    \end{equation}

    This means that the dot product of two vectors is the product of their length
    times the cosine of the angle between them. The angle between $u$ and $v$ is
    given by

    \begin{equation}
        \begin{split}
            \theta - \phi = \arccos{\left(\frac{u\cdot v}{|u| |v|}\right)}
        \end{split}
    \end{equation}

\end{example}

Well, this rises a question. How can you derive the inner product for a real
vector space? Well, there are various points to note, but let's imagine that we
want to measure the length of a vector. How can we measure distance? But also,
we want to measure the distance between two vectors. Let's go with an example
to make things clearer.

\begin{example}[Distance of two vectors in $\mathbb{R}^{n}$]
    We first define two vectors $u=\left(u_1,u_2,\dots,u_n\right)$ and $v=\left(v_1, v-2, \dots, v_n\right)$

    Now, a third vector, we call it $w=u-v$ has squared length
    \begin{equation}
        \begin{split}
            |w|^{2} = (u_1-v_1)^{2} + (u_2-v_2)^{2} + \cdots + (u_n-v_n)^{2}
        \end{split}
    \end{equation}

    Expanding one of the right-hand side terms we get $\left(u_i-v_i\right)^{2} =
        u_i^{2} - 2 v_i u_i + v_i^{2}$. Grouping the terms in (2.4) results in
    \begin{equation}
        \begin{split}
            \sum_{i=1}^{n}{\left(w_i^{2}\right)} = \sum_{i=1}^{n}{\left(u_i^{2}\right)} + \sum_{i=1}^{n}{\left(v_i^{2}\right)} - 2 \sum_{i=1}^{n}{u_i v_i}
        \end{split}
    \end{equation}

    Note that in (2.5) the dot product appears in the last term of the right-hand
    side. We can rewrite the equation as
    \begin{equation}
        \begin{split}
            w \cdot w = v \cdot v + u \cdot u - 2u \cdot v
        \end{split}
    \end{equation}

    And using formula (2.2)
    \begin{equation}
        \begin{split}
            |w|^{2} = |v|^{2} + |u|^{2} - 2|u||v|\cos\theta
        \end{split}
    \end{equation}

    Where $\theta$ is the angle between $u$ and $v$. Note that, because the angle
    between a vector and itself is $\theta = 0$, $\cos\theta = 1$.
\end{example}

Equation (2.7) is nothing more than the \textit{Law of Cosines}. Now, the dot
product does not follow a ``natural pattern'' as one would call it. Think of
the exponential function, it has a very natural reasoning, for example, in the
growth of populations or in differential equations. However, the dot product is
present when we measure elements in Euclidean spaces, like segments or vectors.

The dot product is not ``derived'' in a way most things are. Instead, it is
useful because it simply ``appears'' in measurements.

The \textbf{inner product} is a generalization of the dot product in more
general spaces. Each space can have a different definition for its inner
product. For example

    [Inner product of a functional space $C(a,b)$]

Let $f:\mathbb{R} \to \mathbb{R}$ and $g:\mathbb{R} \to \mathbb{R}$ be
continuous functions in an interval $[a,b]$, the inner product is defined as
\begin{equation}
    \begin{split}
        \left(f,g\right) = \int_{a}^{b}{f(x)g(x)dx}
    \end{split}
\end{equation}

\section{Norms and length}

The norm of an element $x$ in a linear space is written as $\|x\|$ and has the
following properties:

\begin{enumerate}
    \item $\|x\| > 0$ if $x\neq 0$
    \item $\|x\| = 0$ if $x = 0$
    \item For a scalar $a$, $\|ax\| = |a|\cdot \|x\|$
    \item For two elements $y$ and $x$ in a linear space, $\|x+y\| \leq \|x\| + \|y\|$
\end{enumerate}

The $4^{th}$ property is the triangle inequality.

\begin{definition}
    Let $x$ be an element of a linear space $V$.
    The norm of $x$ is defined as $\left(x,x\right)^{1/2}$, this is,
    the square root of the inner product of $x$ with itself.
\end{definition}

Definition (2.3) satisfies the properties of a norm.

\begin{example}
    Let $x\in\mathbb{R}^{n}$, the norm of a vector is given by the pythagorean theorem
    \begin{equation*}
        \begin{split}
            \|x\| = \sqrt{x_1^{2} + x_2^{2} + \cdots + x_n^{2}}
            = \sqrt{\sum_{i=1}^{n}{x_i^{2}}}
        \end{split}
    \end{equation*}

    Finally, we can see that $\|x\|=\sqrt{\left(x,x\right)}$, we defined the inner
    product of a real vector space as the dot product $\left(\cdot, \cdot\right):
        V\times V \to \mathbb{R}$. Here $V$ denotes the linear space, in our current
    example $V=\mathbb{R}^{n}$.
\end{example}

\begin{example}
    Let $V$ be the functional space $C(a,b)$, the norm of a function $f$ the interval $\left[a,b\right]$ is

    \begin{equation*}
        \begin{split}
            \|f\| = \sqrt{\int_{a}^{b}{\left[f(\psi)\right]^{2}d\psi}}
        \end{split}
    \end{equation*}

    This measure in functional spaces are useful when dealing with negative values
    on the integral. For example, the $\sin (\psi)$ function is zero when
    integrated in its period. However we can use the norm to measure it:

    \begin{equation*}
        \begin{split}
            & \|\sin(\psi)\| = \left(\sin(\psi), \sin(\psi)\right)
            \\      & = \sqrt{\int_{\theta_1}^{\theta_2}{\left[\sin(\psi)\right]^{2}d\psi}}
            \\      & = \sqrt{\left[\frac{\psi}{2} - \frac{\sin\left(2\psi\right)}{4}\right]_{\theta_1}^{\theta_2}}
            \\ &= \sqrt{\frac{\theta_1 + \theta_2}{2} - \frac{\sin(\theta_2)\cos(\theta_2)-\sin(\theta_1)\cos(\theta_1)}{2}}
        \end{split}
    \end{equation*}

    If $\theta_1=0$ and $\theta_2=2\pi$ we have $\|\sin(\psi)\|=\sqrt{\pi}$.
\end{example}

\chapter{Projections onto subspaces}

A projection is an idempotent mapping of a set (or any structure) into a subset
(or sub-structure). Idempotent means that, projecting once is the same as
projecting $n$-times.

We will see this as a vector being projected in a line, a line is a subspace of
$\mathbb{R}$, and our vector is in $\mathbb{R}^{2}$.

\begin{center}
    \begin{tikzpicture}
        \coordinate (v) at (2,3);
        \coordinate (e) at (14/5, 7/5);
        \coordinate (l1) at (-2,-1);
        \coordinate (l2) at (4,2);

        \draw[thick, dashed] (l1)--(l2);
        \draw[->, thick] (0,0)--(0,5);
        \draw[->, thick] (0,0)--(5,0);
        \draw[->, very thick] (0,0)--(v);
        \draw[dashed] (v)--(e);
        \draw[->, very thick] (0,0)--(e);

        \draw (l2) + (0.2,0.2) node{$l$};
        \draw (e) + (0, 1) node{$e$};
        \draw (v) + (-1, -1) node{$v$};
        \draw (e) + (-.9, -.9) node{$proj_u {v}$};

    \end{tikzpicture}
\end{center}

This is the most basic example of projection. Let's deduce a formula.

\begin{example}
    Let $v\in\mathbb{R}^{2}$ and let $l$ be a line
    in $\mathbb{R}^{2}$ parametrized by a vector $u$ as $f(t)=ut$.

    \begin{center}
        \begin{tikzpicture}
            \coordinate (v) at (2,3);
            \coordinate (e) at (14/15, 7/15);
            \coordinate (l1) at (0,0);
            \coordinate (l2) at (4,2);

            \draw[thick, dashed] (l1)--(l2);
            \draw[->, thick] (0,0)--(0,5);
            \draw[->, thick] (0,0)--(5,0);
            \draw[->, very thick] (0,0)--(v);
            \draw[->, very thick] (0,0)--(e);

            \draw (l2) + (0.2,0.2) node{$l$};
            \draw (v) + (-1, -1) node{$v$};
            \draw (e) + (.1,.3) node{$u$};

        \end{tikzpicture}
    \end{center}

    What we want to find is some constant $k$ such that $ku$ is perpendicular to
    $l$. We know from the previous image that $e=proj_u v - v = uk-v$

    \begin{equation*}
        \begin{split}
            (ku-v)\cdot u = k\|u\|^{2} - v\cdot u = 0
        \end{split}
    \end{equation*}

    Then $k=\frac{v\cdot u}{\|u\|^{2}}$. The formula for projection becomes

    \begin{equation}
        \begin{split}
            proj_u v = ku = \frac{v\cdot u}{\|u\|^{2}}\cdot u
        \end{split}
    \end{equation}

    We call this projection in terms of $u$, because $l$ is parametrized by $u$.
\end{example}

Now, we want to extend this idea to any linear space. We can see that a
projection is a multiplication of the ``projection basis'' by some scalar.

This gives us an idea of what the inner product is: we can think of the inner
product as ``how much of some element is onto other element''. Think of
projecting a vector $v$ onto $u$ when $v\perp u$.

\begin{center}
    \begin{tikzpicture}
        \coordinate (v) at (2,0);
        \coordinate (e) at (0,2);
        \coordinate (c) at (0.778836684617, 1.84212198801);
        \coordinate (d) at (0.39733866159, 1.96013315568);

        \draw[->] (0,0)--(v);
        \draw[->] (0,0)--(e);
        \draw[->] (0,0)--(c);
        \draw[->] (0,0)--(d);
        \draw[dotted] (0.39733866159,0) -- (d);
        \draw[dotted] (0.778836684617,0) -- (c);
        \draw[->, thick] (0,0) -- (0.778836684617,0);
        \draw[->, thick] (0,0) -- (0.39733866159,0);
    \end{tikzpicture}
\end{center}

You can see that the projection tends to a vector with no length. This can be
proven easily with (2.2):

\begin{equation}
    \begin{split}
        proj_u v = u\cdot\frac{u\cdot v}{\|u\|^{2}}
        \\ = u\cdot\frac{\|v\|}{\|u\|} \cos\psi
    \end{split}
\end{equation}

The $\frac{\|v\|}{\|u\|}$ part is the amount of times $u$ fits in $v$, and if
$\psi = \frac{\pi}{2}$ then the projection is the $O$ vector.

\begin{example}[Projections in $\mathbb{R}^{n}$]
    Let $x\in\mathbb{R}^{n}$ be a vector in a plane, this plane must be of dimension $n-1$.
    Thus, it has a basis with $n-1$ elements given by
    \begin{equation*}
        W = \left(e_1, e_2,\dots, e_{n-1}\right)
    \end{equation*}

    Where $e_1,e_2,\dots,e_{n-1}\in\mathbb{R}^{n}$. These vectors parametrize the
    plane, such that $x$ can be written as
    \begin{equation}
        \begin{split}
            x = \sum_{i=1}^{n-1}{c_i e_i}
        \end{split}
    \end{equation}

    for any scalars $c_1,c_2,\dots,c_{n-1}$. Now, suppose that we have a vector
    $v\in\mathbb{R}^{n}$ that preferably, does not lie in this plane.

    To find the projection, we can set a set of equation that are analogous to the
    example in $\mathbb{R}^{2}$:
    \begin{equation}
        \begin{split}
            \begin{cases}
                e_1(v - k x) = 0 \\
                e_2(v - k x) = 0 \\
                \cdots           \\
                e_{n-1}(v - k x) = 0
            \end{cases}
        \end{split}
    \end{equation}

    Where $k\in\mathbb{R}$. What we want is to make each basis vector orthogonal to
    a vector $x$ that connects $v$ to the plane. In other words, the vector
    $(v-kx)$ will be perpendicular to the plane.

    We will collect each basis vector into a matrix
    \begin{equation}
        A = \left[
            \begin{matrix}
                \vdots & \vdots &        & \vdots  \\
                e_1    & e_2    & \cdots & e_{n-1} \\
                \vdots & \vdots &        & \vdots  \\
            \end{matrix}
            \right]
    \end{equation}

    Here, $A$ has dimension $n\times (n-1)$. The system of equations in (3.4)
    becomes
    \begin{equation}
        A^{T}\left(v-kx\right) = 0
    \end{equation}

    But we can rewrite $x$ as a vector $c$ times $A$, so that
    \begin{equation}
        x = Ac =  \left[\begin{matrix}
                \vdots & \vdots &        & \vdots  \\
                e_1    & e_2    & \cdots & e_{n-1} \\
                \vdots & \vdots &        & \vdots  \\
            \end{matrix}\right]\cdot\left[\begin{matrix}
                c_1 \\c_2\\\vdots\\c_{n-1}
            \end{matrix}\right]
    \end{equation}

    You can verify that this is the same as writing $x$ in the form (3.3).
    Rewriting equation (3.6) leaves
    \begin{equation*}
        A^{T}\left(v-Ac\right) = 0
    \end{equation*}

    We omit the term $k$ since we will assume that the vector $c$ scales $x$
    accordingly. Expanding the equation
    \begin{equation}
        \begin{split}
            A^{T}\left(v-Ac\right) = A^{T}v - A^{T}Ac = 0
        \end{split}
    \end{equation}

    Now, we solve for $c$
    \begin{equation}
        \begin{split}
            c={(A^{T}A)}^{-1}A^{T}v
        \end{split}
    \end{equation}

    Now, as the projection is given by scaling all the basis vectors in $A$ by $c$
    (think of $c$ as the scalar term in the 2-dimensional example), we can write
    our projection vector $p$ as

    \begin{equation}
        p = Ac = A{(A^{T}A)}^{-1}A^{T}v = Pv
    \end{equation}

    We define $P$ to be the \textbf{projection matrix}, and $v$ is the vector
    projected onto the plane $A$.

    To figure out the equation of the plane, we think of the definition of a plane:
    each vector in the plane must be perpendicular to a normal vector. This means
    that we always have $A\cdot n = O$, where $n$ is the normal vector.

    We can write the equation for an hyperplane in $\mathbb{R}^{n-1}$ as
    \begin{equation}
        n^{T}(x-x_0) = 0
    \end{equation}

    where:
    \begin{itemize}
        \item $n \in \mathbb{R}^n$ is the \textbf{normal vector} to the hyperplane,
        \item $x \in \mathbb{R}^n$ is any point \textbf{on} the hyperplane,
        \item $x_0 \in \mathbb{R}^n$ is a fixed point in the hyperplane (used to position or ``anchor'' the hyperplane in space).
    \end{itemize}

    This equation states that the vector from $x_0$ to any point $x$ on the
    hyperplane is \textbf{orthogonal} to the normal vector $n$, ensuring all such
    points lie in the same flat $(n-1)$-dimensional space.

    In this definition we didn't use the basis vectors in $W$, however as $x$ is a
    combination of basis vectors, it is not necessary to explicitly write them.
\end{example}

\end{document}